üîπ Key Metrics for Accuracy Measurement
‚úÖ 1Ô∏è‚É£ Precision & Recall (Information Retrieval Metrics)

Precision ‚Üí What percentage of retrieved resumes are actually relevant?

Recall ‚Üí What percentage of relevant resumes were successfully retrieved?

Formula:

plaintext
Precision = (Relevant CVs Retrieved) / (Total CVs Retrieved)  
Recall = (Relevant CVs Retrieved) / (Total Relevant CVs in Dataset)  
‚úÖ 2Ô∏è‚É£ Mean Reciprocal Rank (MRR) - Ranking Quality

Measures the ranking quality of retrieved resumes based on HR feedback.

Formula:

plaintext
MRR = 1 / Rank of First Relevant Resume  
Example Interpretation: If the first relevant resume appears at rank 2, MRR = 1/2 = 0.5 (higher values are better).

‚úÖ 3Ô∏è‚É£ Cosine Similarity Between Job Description & Resumes

Measures textual similarity using embeddings

Higher cosine scores mean better matches

Python Example:

python
from sklearn.metrics.pairwise import cosine_similarity

job_desc_embedding = sentence_model.encode(job_description).reshape(1, -1)
resume_embeddings = sentence_model.encode(resume_texts).reshape(len(resume_texts), -1)

similarities = cosine_similarity(job_desc_embedding, resume_embeddings)
print("Resume Similarity Scores:", similarities)
‚úÖ 4Ô∏è‚É£ Human Evaluation (HR Feedback)

Ask recruiters to manually rate retrieved resumes from 1-5 based on relevance.

Use this feedback to fine-tune filtering thresholds & rerank results.

‚úÖ 5Ô∏è‚É£ False Positive & False Negative Rates

False Positives (irrelevant resumes retrieved) ‚Üí Can affect hiring decisions.

False Negatives (missed relevant resumes) ‚Üí Can cause good candidates to be overlooked.

Formula:

plaintext
False Positive Rate = FP / (FP + TN)
False Negative Rate = FN / (FN + TP)